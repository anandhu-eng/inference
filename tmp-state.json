{
  "CM_HW_META": {
    "accelerator_frequency": "2520.000000 MHz",
    "accelerator_host_interconnect": "N/A",
    "accelerator_interconnect": "N/A",
    "accelerator_interconnect_topology": "",
    "accelerator_memory_capacity": "23.64288330078125 GB",
    "accelerator_memory_configuration": "N/A",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerator_on-chip_memories": "",
    "accelerators_per_node": "1",
    "cooling": "air",
    "division": "open",
    "framework": "pytorch",
    "host_memory_capacity": "134G",
    "host_memory_configuration": "undefined",
    "host_network_card_count": "1",
    "host_networking": "Gig Ethernet",
    "host_networking_topology": "N/A",
    "host_processor_caches": "L1d cache: 896 KiB (24 instances), L1i cache: 1.3 MiB (24 instances), L2 cache: 32 MiB (12 instances), L3 cache: 36 MiB (1 instance)",
    "host_processor_core_count": "24",
    "host_processor_frequency": "5800.0000",
    "host_processor_interconnect": "",
    "host_processor_model_name": "13th Gen Intel(R) Core(TM) i9-13900K",
    "host_processors_per_node": "1",
    "host_storage_capacity": "6.4T",
    "host_storage_type": "SSD",
    "hw_notes": "",
    "number_of_nodes": "1",
    "operating_system": "Ubuntu 24.04 (linux-6.8.0-31-generic-glibc2.39)",
    "other_software_stack": "Python: 3.12.3, GCC-13.2.0",
    "status": "available",
    "submitter": "CTuning",
    "sw_notes": "",
    "system_name": "intel_spr_i9 (auto detected)",
    "system_type": "edge",
    "system_type_detail": "edge server"
  },
  "CM_SUT_CONFIG": {
    "intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config": {
      "3d-unet-99": {
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 500
        }
      },
      "3d-unet-99.9": {
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 500
        }
      },
      "bert-99": {
        "Offline": {
          "target_qps": "91.7037"
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 1
        }
      },
      "bert-99.9": {
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        }
      },
      "gpt-j": {
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 500
        }
      },
      "gptj-99": {
        "Offline": {
          "target_qps": "0.672856"
        },
        "SingleStream": {
          "target_latency": "2092.7362860000003"
        }
      },
      "llama2-70b-99": {
        "Offline": {
          "target_qps": 0.1
        },
        "Server": {
          "target_qps": 0.1
        },
        "SingleStream": {
          "target_latency": 2000
        }
      },
      "llama2-70b-99.9": {
        "Offline": {
          "target_qps": 0.1
        },
        "Server": {
          "target_qps": 0.1
        },
        "SingleStream": {
          "target_latency": 2000
        }
      },
      "resnet50": {
        "MultiStream": {
          "target_latency": 0.1
        },
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 0.1
        }
      },
      "retinanet": {
        "MultiStream": {
          "target_latency": 1
        },
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 1
        }
      },
      "sdxl": {
        "Offline": {
          "target_qps": 1.0
        },
        "Server": {
          "target_qps": 1.0
        },
        "SingleStream": {
          "target_latency": 200
        }
      }
    }
  },
  "CM_SUT_CONFIG_NAME": "intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config",
  "CM_SUT_CONFIG_PATH": {
    "intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config": "/home/anandhu/CM/repos/local/cache/baf26f8879624386/intel_spr_i9/reference-implementation/gpu-device/pytorch-framework/framework-version-v2.3.0/default_config-config.yaml"
  },
  "CM_SUT_META": {
    "accelerator_frequency": "2520.000000 MHz",
    "accelerator_host_interconnect": "N/A",
    "accelerator_interconnect": "N/A",
    "accelerator_interconnect_topology": "",
    "accelerator_memory_capacity": "23.64288330078125 GB",
    "accelerator_memory_configuration": "N/A",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerator_on-chip_memories": "",
    "accelerators_per_node": "1",
    "cooling": "air",
    "division": "open",
    "framework": "pytorch",
    "host_memory_capacity": "134G",
    "host_memory_configuration": "undefined",
    "host_network_card_count": "1",
    "host_networking": "Gig Ethernet",
    "host_networking_topology": "N/A",
    "host_processor_caches": "L1d cache: 896 KiB (24 instances), L1i cache: 1.3 MiB (24 instances), L2 cache: 32 MiB (12 instances), L3 cache: 36 MiB (1 instance)",
    "host_processor_core_count": "24",
    "host_processor_frequency": "5800.0000",
    "host_processor_interconnect": "",
    "host_processor_model_name": "13th Gen Intel(R) Core(TM) i9-13900K",
    "host_processors_per_node": "1",
    "host_storage_capacity": "6.4T",
    "host_storage_type": "SSD",
    "hw_notes": "",
    "number_of_nodes": "1",
    "operating_system": "Ubuntu 24.04 (linux-6.8.0-31-generic-glibc2.39)",
    "other_software_stack": "Python: 3.12.3, GCC-13.2.0",
    "status": "available",
    "submitter": "CTuning",
    "sw_notes": "",
    "system_name": "intel_spr_i9 (auto detected)",
    "system_type": "edge",
    "system_type_detail": "edge server"
  },
  "RUN": {
    "SingleStream": {}
  },
  "cm_cuda_device_prop": {
    "CUDA driver version": "12.4",
    "CUDA runtime version": "12.4",
    "GPU Device ID": "0",
    "GPU Name": "NVIDIA GeForce RTX 4090",
    "GPU compute capability": "8.9",
    "Global memory": "25386352640",
    "Max clock rate": "2520.000000 MHz",
    "Max dimension size of a grid size X": "2147483647",
    "Max dimension size of a grid size Y": "65535",
    "Max dimension size of a grid size Z": "65535",
    "Max dimension size of a thread block X": "1024",
    "Max dimension size of a thread block Y": "1024",
    "Max dimension size of a thread block Z": "64",
    "Maximum number of threads per block": "1024",
    "Maximum number of threads per multiprocessor": "1536",
    "Total amount of shared memory per block": "49152",
    "Total number of registers available per block": "65536",
    "Warp size": "32"
  },
  "docker": {},
  "mlperf-inference-implementation": {
    "script_id": "app-mlperf-inference,d775cac873ee4231:reference,gptj-99,pytorch,cuda,valid,bfloat16,singlestream"
  },
  "mlperf_inference_run_cmd": "cm run script --tags=run-mlperf,inference,_performance-only --model=gptj-99 --backend=pytorch --device=cuda --precision=bfloat16 --rerun --quiet --env.GPTJ_BEAM_SIZE=2 --scenario=SingleStream --adr.compiler.tags=gcc --adr.mlperf-implementation.tags=_repo.https://github.com/anandhu-eng/inference/,_branch.GPT-J --adr.cuda.version=12.4.1 --execution-mode=valid",
  "os_uname_all": "Linux intel-spr-i9 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux",
  "os_uname_machine": "x86_64"
}
