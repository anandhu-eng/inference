
Checking compiler version ...

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0

Compiling program ...


Running program ...

/home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference

* cm run script "run-mlperf inference _performance-only"

  * cm run script "detect os"
         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py

  * cm run script "detect cpu"

    * cm run script "detect os"
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/customize.py

  * cm run script "get python3"
       ! load /home/anandhu/CM/repos/local/cache/43c0b6203e24414d/cm-cached-state.json

Path to Python: /usr/bin/python3
Python version: 3.12.3


  * cm run script "get mlcommons inference src"
       ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json

  * cm run script "get sut description"

    * cm run script "detect os"
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py

    * cm run script "detect cpu"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/customize.py

    * cm run script "get python3"
         ! load /home/anandhu/CM/repos/local/cache/43c0b6203e24414d/cm-cached-state.json

Path to Python: /usr/bin/python3
Python version: 3.12.3


    * cm run script "get compiler gcc"
         ! load /home/anandhu/CM/repos/local/cache/ac761d8b65524792/cm-cached-state.json

    * cm run script "get cuda-devices"

      * cm run script "get cuda _toolkit"

        * cm run script "detect os"
               ! cd /home/anandhu/CM/repos/local/cache/7586c05eaf884a6e
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
               ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
          # Requested paths: /home/anandhu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/cuda/bin:/usr/local/cuda-11/bin:/usr/cuda-11/bin:/usr/local/cuda-12/bin:/usr/cuda-12/bin:/usr/local/packages/cuda
        - Searching for versions:  == 12.4.1

          * /usr/bin/nvcc
                 ! cd /home/anandhu/CM/repos/local/cache/7586c05eaf884a6e
                 ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
                 ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
          Detected version: 12.0
          SKIPPED due to version constraints ...

          * /usr/local/cuda/bin/nvcc
                 ! cd /home/anandhu/CM/repos/local/cache/7586c05eaf884a6e
                 ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
                 ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
          Detected version: 12.4
          SKIPPED due to version constraints ...

        * cm run script "install cuda prebuilt"
             ! load /home/anandhu/CM/repos/local/cache/a31c402a715546bb/cm-cached-state.json
             ! cd /home/anandhu/CM/repos/local/cache/7586c05eaf884a6e
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
          Detected version: 12.4

ENV[CM_CUDA_PATH_LIB_CUDNN_EXISTS]: no
ENV[CM_CUDA_VERSION]: 12.4
ENV[CM_CUDA_VERSION_STRING]: cu124
ENV[CM_NVCC_BIN_WITH_PATH]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install/bin/nvcc
ENV[CUDA_HOME]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install

           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda-devices/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda-devices/customize.py
GPU Device ID: 0
GPU Name: NVIDIA GeForce RTX 4090
GPU compute capability: 8.9
CUDA driver version: 12.4
CUDA runtime version: 12.4
Global memory: 25386352640
Max clock rate: 2520.000000 MHz
Total amount of shared memory per block: 49152
Total number of registers available per block: 65536
Warp size: 32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block: 1024
Max dimension size of a thread block X: 1024
Max dimension size of a thread block Y: 1024
Max dimension size of a thread block Z: 64
Max dimension size of a grid size X: 2147483647
Max dimension size of a grid size Y: 65535
Max dimension size of a grid size Z: 65535



    * cm run script "get generic-python-lib _package.dmiparser"
         ! load /home/anandhu/CM/repos/local/cache/67cc9145168b4481/cm-cached-state.json

    * cm run script "get cache dir _name.mlperf-inference-sut-descriptions"
         ! load /home/anandhu/CM/repos/local/cache/4dbd0fedebae4015/cm-cached-state.json
Generating SUT description file for intel_spr_i9-pytorch
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-mlperf-inference-sut-description/customize.py

  * cm run script "get mlperf inference results dir"
       ! load /home/anandhu/CM/repos/local/cache/e9763054e67d477d/cm-cached-state.json

  * cm run script "install pip-package for-cmind-python _package.tabulate"
       ! load /home/anandhu/CM/repos/local/cache/69f099dcb8514596/cm-cached-state.json

  * cm run script "get mlperf inference utils"

    * cm run script "get mlperf inference src"
         ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-mlperf-inference-utils/customize.py
Using MLCommons Inference source from /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference

Running loadgen scenario: SingleStream and mode: performance

* cm run script "app mlperf inference generic _reference _gptj-99 _pytorch _cuda _valid _bfloat16 _singlestream"

  * cm run script "detect os"
         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py

  * cm run script "get sys-utils-cm"
       ! load /home/anandhu/CM/repos/local/cache/f238419d15d349b5/cm-cached-state.json

  * cm run script "get python"
       ! load /home/anandhu/CM/repos/local/cache/43c0b6203e24414d/cm-cached-state.json

Path to Python: /usr/bin/python3
Python version: 3.12.3


  * cm run script "get mlcommons inference src"
       ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json

  * cm run script "get mlperf inference utils"


Checking compiler version ...

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0

Compiling program ...


Running program ...

/home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
    * cm run script "get mlperf inference src"
         ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-mlperf-inference-utils/customize.py

  * cm run script "get cuda-devices"

    * cm run script "get cuda _toolkit"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/a3a0151fc93e49ff
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
        # Requested paths: /home/anandhu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/cuda/bin:/usr/local/cuda-11/bin:/usr/cuda-11/bin:/usr/local/cuda-12/bin:/usr/cuda-12/bin:/usr/local/packages/cuda
      - Searching for versions:  == 12.4.1

        * /usr/bin/nvcc
               ! cd /home/anandhu/CM/repos/local/cache/a3a0151fc93e49ff
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
               ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.0
        SKIPPED due to version constraints ...

        * /usr/local/cuda/bin/nvcc
               ! cd /home/anandhu/CM/repos/local/cache/a3a0151fc93e49ff
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
               ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.4
        SKIPPED due to version constraints ...

      * cm run script "install cuda prebuilt"
           ! load /home/anandhu/CM/repos/local/cache/a31c402a715546bb/cm-cached-state.json
           ! cd /home/anandhu/CM/repos/local/cache/a3a0151fc93e49ff
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.4

ENV[CM_CUDA_PATH_LIB_CUDNN_EXISTS]: no
ENV[CM_CUDA_VERSION]: 12.4
ENV[CM_CUDA_VERSION_STRING]: cu124
ENV[CM_NVCC_BIN_WITH_PATH]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install/bin/nvcc
ENV[CUDA_HOME]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install

         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda-devices/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda-devices/customize.py
GPU Device ID: 0
GPU Name: NVIDIA GeForce RTX 4090
GPU compute capability: 8.9
CUDA driver version: 12.4
CUDA runtime version: 12.4
Global memory: 25386352640
Max clock rate: 2520.000000 MHz
Total amount of shared memory per block: 49152
Total number of registers available per block: 65536
Warp size: 32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block: 1024
Max dimension size of a thread block X: 1024
Max dimension size of a thread block Y: 1024
Max dimension size of a thread block Z: 64
Max dimension size of a grid size X: 2147483647
Max dimension size of a grid size Y: 65535
Max dimension size of a grid size Z: 65535



  * cm run script "app mlperf reference inference _cuda _singlestream _gptj-99 _pytorch _bfloat16"

    * cm run script "detect os"
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py

    * cm run script "detect cpu"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/customize.py

    * cm run script "get sys-utils-cm"
         ! load /home/anandhu/CM/repos/local/cache/f238419d15d349b5/cm-cached-state.json

    * cm run script "get python"
         ! load /home/anandhu/CM/repos/local/cache/43c0b6203e24414d/cm-cached-state.json

Path to Python: /usr/bin/python3
Python version: 3.12.3


    * cm run script "get cuda _cudnn"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/7ec3341f084b467c
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
        # Requested paths: /home/anandhu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/cuda/bin:/usr/local/cuda-11/bin:/usr/cuda-11/bin:/usr/local/cuda-12/bin:/usr/cuda-12/bin:/usr/local/packages/cuda
      - Searching for versions:  == 12.4.1

        * /usr/bin/nvcc
               ! cd /home/anandhu/CM/repos/local/cache/7ec3341f084b467c
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
               ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.0
        SKIPPED due to version constraints ...

        * /usr/local/cuda/bin/nvcc
               ! cd /home/anandhu/CM/repos/local/cache/7ec3341f084b467c
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
               ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.4
        SKIPPED due to version constraints ...

      * cm run script "install cuda prebuilt"
           ! load /home/anandhu/CM/repos/local/cache/a31c402a715546bb/cm-cached-state.json
           ! cd /home/anandhu/CM/repos/local/cache/7ec3341f084b467c
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.4

    * cm run script "get nvidia cudnn"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/4efebe580f8d4d15
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
        # Requested paths: /home/anandhu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/cuda/bin:/usr/local/cuda-11/bin:/usr/cuda-11/bin:/usr/local/cuda-12/bin:/usr/cuda-12/bin:/usr/local/packages/cuda:/usr/local/cuda/lib64:/usr/cuda/lib64:/usr/local/cuda/lib:/usr/cuda/lib:/usr/local/cuda-11/lib64:/usr/cuda-11/lib:/usr/local/cuda-12/lib:/usr/cuda-12/lib:/usr/local/packages/cuda/lib:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/lib64:/usr/lib64:/usr/local/lib:/lib:/usr/lib

        # Found artifact in /lib/x86_64-linux-gnu/libcudnn.so
           ! cd /home/anandhu/CM/repos/local/cache/4efebe580f8d4d15
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cudnn/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cudnn/customize.py

ENV[CM_CUDA_PATH_INCLUDE_CUDNN]: /usr/include
Loading PyTorch model...
BF16 autocast
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.20it/s]
/home/anandhu/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Casting models to GPU...
  0%|          | 0/285 [00:00<?, ?it/s]100%|██████████| 285/285 [00:00<00:00, 2615703.81it/s]
================================================
MLPerf Results Summary
================================================
SUT name : PySUT
Scenario : SingleStream
Mode     : PerformanceOnly
90th percentile latency (ns) : 3559372575
Result is : INVALID
  Min duration satisfied : NO
  Min queries satisfied : NO
  Early stopping satisfied: Yes
Recommendations:
 * Decrease the expected latency so the loadgen pre-generates more queries.
 * The test exited early, before enough queries were issued.
   See the detailed log for why this may have occurred.
Early Stopping Result:
 * Processed at least 64 queries (100).
 * Would discard 2 highest latency queries.
 * Early stopping 90th percentile estimate: 4124959936
 * Not enough queries processed for 99th percentile
 early stopping estimate (would need to process at
 least 662 total queries).

================================================
Additional Stats
================================================
QPS w/ loadgen overhead         : 0.46
QPS w/o loadgen overhead        : 0.46

Min latency (ns)                : 965628617
Max latency (ns)                : 4882868934
Mean latency (ns)               : 2174236568
50.00 percentile latency (ns)   : 1958893850
90.00 percentile latency (ns)   : 3559372575
95.00 percentile latency (ns)   : 3940767283
97.00 percentile latency (ns)   : 4124959936
99.00 percentile latency (ns)   : 4882868934
99.90 percentile latency (ns)   : 4882868934

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 0.510939
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 13368
max_query_count : 100
qsl_rng_seed : 13281865557512327830
sample_index_rng_seed : 198141574272810017
schedule_rng_seed : 7575108116881280410
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 13368

No warnings encountered during test.

1 ERROR encountered. See detailed log.
Constructing QSL
Encoding Samples
Finished constructing QSL.
Constructing QSL
Encoding Samples
Finished constructing QSL.
Finished constructing QDL!
Running LoadGen test...
Completed :  5
Completed :  10
Completed :  15
Completed :  20
Completed :  25
Completed :  30
Completed :  35
Completed :  40
Completed :  45
Completed :  50
Completed :  55
Completed :  60
Completed :  65
Completed :  70
Completed :  75
Completed :  80
Completed :  85
Completed :  90
Completed :  95
Completed :  100
Test Done!
Destroying SUT...
Destroying QSL...
ENV[CM_CUDA_PATH_LIB_CUDNN]: /lib/x86_64-linux-gnu
ENV[CM_CUDNN_VERSION]: 9.1.1


ENV[CM_CUDA_PATH_LIB_CUDNN_EXISTS]: yes
ENV[CM_CUDA_VERSION]: 12.4
ENV[CM_CUDA_VERSION_STRING]: cu124
ENV[CM_NVCC_BIN_WITH_PATH]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install/bin/nvcc
ENV[CUDA_HOME]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install


    * cm run script "get generic-python-lib _torch"
         ! load /home/anandhu/CM/repos/local/cache/70259d08de2e4414/cm-cached-state.json

    * cm run script "get generic-python-lib _torch_cuda"
         ! load /home/anandhu/CM/repos/local/cache/23fe41549e0b49b2/cm-cached-state.json

    * cm run script "get generic-python-lib _torchvision_cuda"
         ! load /home/anandhu/CM/repos/local/cache/4dcd0b2b50064c87/cm-cached-state.json

    * cm run script "get generic-python-lib _transformers"
         ! load /home/anandhu/CM/repos/local/cache/6bbdd69058b74ed1/cm-cached-state.json

    * cm run script "get ml-model large-language-model gptj raw _fp32 _pytorch"
         ! load /home/anandhu/CM/repos/local/cache/3c9ad769df8a4119/cm-cached-state.json

Path to the ML model: /home/anandhu/CM/repos/local/cache/98ec1ceb760d474a/checkpoint/checkpoint-final


    * cm run script "get dataset cnndm _validation"
         ! load /home/anandhu/CM/repos/local/cache/a09838a739894dbd/cm-cached-state.json

    * cm run script "generate user-conf mlperf inference"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py

      * cm run script "detect cpu"

        * cm run script "detect os"
               ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
               ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
             ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/customize.py

      * cm run script "get python"
           ! load /home/anandhu/CM/repos/local/cache/43c0b6203e24414d/cm-cached-state.json

Path to Python: /usr/bin/python3
Python version: 3.12.3


      * cm run script "get mlcommons inference src"
           ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json

      * cm run script "get sut configs"

        * cm run script "get cache dir _name.mlperf-inference-sut-configs"
             ! load /home/anandhu/CM/repos/local/cache/baf26f8879624386/cm-cached-state.json
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-mlperf-inference-sut-configs/customize.py
Using MLCommons Inference source from '/home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference'
Original configuration value 2174.645193 target_latency
Adjusted configuration value 1957.1806737 target_latency
Output Dir: '/home/anandhu/CM/repos/local/cache/e9763054e67d477d/valid_results/intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config/gptj-99/singlestream/performance/run_1'
gptj.SingleStream.target_latency = 1957.1806737
gptj.SingleStream.max_query_count = 100


    * cm run script "get loadgen"
         ! load /home/anandhu/CM/repos/local/cache/813dcefa499444e2/cm-cached-state.json

Path to the tool: /home/anandhu/CM/repos/local/cache/813dcefa499444e2/install


    * cm run script "get mlcommons inference src"
         ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json

    * cm run script "get mlcommons inference src _repo.https://github.com/anandhu-eng/inference/ _branch.GPT-J"
         ! load /home/anandhu/CM/repos/local/cache/8bdee82576cd413e/cm-cached-state.json

    * cm run script "get generic-python-lib _package.psutil"
         ! load /home/anandhu/CM/repos/local/cache/1923f1ccb94341b6/cm-cached-state.json

    * cm run script "get generic-python-lib _package.datasets"
         ! load /home/anandhu/CM/repos/local/cache/ac79fd3102164ab8/cm-cached-state.json

    * cm run script "get generic-python-lib _package.attrs"
         ! load /home/anandhu/CM/repos/local/cache/ef0e4786c55544fa/cm-cached-state.json

    * cm run script "get generic-python-lib _package.accelerate"
         ! load /home/anandhu/CM/repos/local/cache/73a4ac1a61d44b05/cm-cached-state.json
Using MLCommons Inference source from '/home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference'
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/app-mlperf-inference-mlcommons-python/customize.py

  * cm run script "benchmark-mlperf"
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/benchmark-program-mlperf/customize.py

  * cm run script "benchmark-program program"

    * cm run script "detect cpu"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/customize.py
***************************************************************************
CM script::benchmark-program/run.sh

Run Directory: /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference/language/gpt-j

CMD: /usr/bin/python3 main.py --model-path=/home/anandhu/CM/repos/local/cache/98ec1ceb760d474a/checkpoint/checkpoint-final --dataset-path=/home/anandhu/CM/repos/local/cache/a09838a739894dbd/install/cnn_eval.json --scenario SingleStream    --mlperf_conf '/home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference/mlperf.conf' --dtype bfloat16 --user_conf '/home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/generate-mlperf-inference-user-conf/tmp/a30ba8194b9c428f84ec8368d1a6b45c.conf' --gpu 2>&1 | tee /home/anandhu/CM/repos/local/cache/e9763054e67d477d/valid_results/intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config/gptj-99/singlestream/performance/run_1/console.out

         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/benchmark-program/run-ubuntu.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/benchmark-program/customize.py

  * cm run script "save mlperf inference state"
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/save-mlperf-inference-implementation-state/customize.py
       ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
       ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/app-mlperf-inference/run.sh from tmp-run.sh
       ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/app-mlperf-inference/customize.py

* cm run script "get mlperf sut description"

  * cm run script "detect os"
         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py

  * cm run script "detect cpu"

    * cm run script "detect os"
           ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh

Checking compiler version ...

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0

Compiling program ...


Running program ...

/home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
Running: 
/usr/bin/python3 /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/dump-pip-freeze/dump.py

           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-cpu/customize.py

  * cm run script "get python3"
       ! load /home/anandhu/CM/repos/local/cache/43c0b6203e24414d/cm-cached-state.json

Path to Python: /usr/bin/python3
Python version: 3.12.3


  * cm run script "get compiler gcc"
       ! load /home/anandhu/CM/repos/local/cache/ac761d8b65524792/cm-cached-state.json

  * cm run script "get cuda-devices"

    * cm run script "get cuda _toolkit"

      * cm run script "detect os"
             ! cd /home/anandhu/CM/repos/local/cache/d5107a2522294a05
             ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/run.sh from tmp-run.sh
             ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/detect-os/customize.py
        # Requested paths: /home/anandhu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/usr/cuda/bin:/usr/local/cuda-11/bin:/usr/cuda-11/bin:/usr/local/cuda-12/bin:/usr/cuda-12/bin:/usr/local/packages/cuda
      - Searching for versions:  == 12.4.1

        * /usr/bin/nvcc
               ! cd /home/anandhu/CM/repos/local/cache/d5107a2522294a05
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
               ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.0
        SKIPPED due to version constraints ...

        * /usr/local/cuda/bin/nvcc
               ! cd /home/anandhu/CM/repos/local/cache/d5107a2522294a05
               ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
               ! call "detect_version" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.4
        SKIPPED due to version constraints ...

      * cm run script "install cuda prebuilt"
           ! load /home/anandhu/CM/repos/local/cache/a31c402a715546bb/cm-cached-state.json
           ! cd /home/anandhu/CM/repos/local/cache/d5107a2522294a05
           ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/run.sh from tmp-run.sh
           ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda/customize.py
        Detected version: 12.4

ENV[CM_CUDA_PATH_LIB_CUDNN_EXISTS]: no
ENV[CM_CUDA_VERSION]: 12.4
ENV[CM_CUDA_VERSION_STRING]: cu124
ENV[CM_NVCC_BIN_WITH_PATH]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install/bin/nvcc
ENV[CUDA_HOME]: /home/anandhu/CM/repos/local/cache/a31c402a715546bb/install

         ! cd /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
         ! call /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda-devices/run.sh from tmp-run.sh
         ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-cuda-devices/customize.py
GPU Device ID: 0
GPU Name: NVIDIA GeForce RTX 4090
GPU compute capability: 8.9
CUDA driver version: 12.4
CUDA runtime version: 12.4
Global memory: 25386352640
Max clock rate: 2520.000000 MHz
Total amount of shared memory per block: 49152
Total number of registers available per block: 65536
Warp size: 32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block: 1024
Max dimension size of a thread block X: 1024
Max dimension size of a thread block Y: 1024
Max dimension size of a thread block Z: 64
Max dimension size of a grid size X: 2147483647
Max dimension size of a grid size Y: 65535
Max dimension size of a grid size Z: 65535



  * cm run script "get generic-python-lib _package.dmiparser"
       ! load /home/anandhu/CM/repos/local/cache/67cc9145168b4481/cm-cached-state.json

  * cm run script "get cache dir _name.mlperf-inference-sut-descriptions"
       ! load /home/anandhu/CM/repos/local/cache/4dbd0fedebae4015/cm-cached-state.json
Generating SUT description file for intel_spr_i9-pytorch-2.3.0
       ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/get-mlperf-inference-sut-description/customize.py


SUT: intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config, model: gptj-99, scenario: SingleStream, target_latency updated as 2174.236568
New config stored in /home/anandhu/CM/repos/local/cache/baf26f8879624386/intel_spr_i9/reference-implementation/gpu-device/pytorch-framework/framework-version-v2.3.0/default_config-config.yaml
intel_spr_i9-reference-gpu-pytorch-v2.3.0-default_config
+---------+--------------+----------+---------+-----------------+
|  Model  |   Scenario   | Accuracy |   QPS   | Latency (in ms) |
+---------+--------------+----------+---------+-----------------+
| gptj-99 | SingleStream |    -     | X 0.242 |    X 4124.96    |
+---------+--------------+----------+---------+-----------------+

The MLPerf inference results are stored at /home/anandhu/CM/repos/local/cache/e9763054e67d477d/valid_results

       ! call "postprocess" from /home/anandhu/CM/repos/anandhu-eng@cm4mlops/script/run-mlperf-inference-app/customize.py

Path to the MLPerf inference benchmark reference sources: /home/anandhu/CM/repos/local/cache/ad4a5758459742c6/inference
Path to the MLPerf inference reference configuration file: /home/anandhu/CM/repos/local/cache/14f34ff81a3145f6/inference/mlperf.conf

